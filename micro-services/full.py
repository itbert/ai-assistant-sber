import feedparser
import sqlite3
import logging
import threading
import torch
from datetime import datetime
from transformers import pipeline, AutoTokenizer, MPNetModel
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BOT_TOKEN = "7642905079:AAEApga21ZloQrAmjdchm_GFugRS0i4C-uU"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
ARXIV_CATEGORIES = ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'stat.ML']
PARSE_INTERVAL = 3600  # 1 —á–∞—Å
DB_FILE = "arxiv_articles.db"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


def init_models():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    tokenizer = AutoTokenizer.from_pretrained("ai-lab/ESGify")
    model = torch.load('esgify_model.pt', map_location=torch.device('cpu'))
    
    return summarizer, tokenizer, model

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS articles
                 (id TEXT PRIMARY KEY, 
                  title TEXT, 
                  summary TEXT,
                  tags TEXT)''')
    conn.commit()
    conn.close()

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –≤ –ë–î
def save_article(article):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("INSERT OR IGNORE INTO articles VALUES (?, ?, ?, ?)",
              (article['id'], article['title'], article['summary'], ','.join(article['tags'])))
    conn.commit()
    conn.close()

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î –ø–æ —Ç–µ–≥—É
def get_articles_by_tag(tag, limit=5):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT id, title, summary FROM articles WHERE tags LIKE ? LIMIT ?", 
             (f'%{tag}%', limit))
    return c.fetchall()

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
def classify_text(text, tokenizer, model):
    inputs = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
        return_attention_mask=True
    )
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    # –ë–µ—Ä–µ–º —Ç–æ–ø-3 —Ç–µ–≥–∞
    top_probs, top_indices = torch.topk(outputs.flatten(), 3)
    tags = [model.id2label[idx.item()] for idx in top_indices]
    return tags

# –ü–∞—Ä—Å–µ—Ä arXiv
def parse_arxiv(summarizer, tokenizer, classifier):
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ arXiv...")
    
    for category in ARXIV_CATEGORIES:
        feed = feedparser.parse(f"http://arxiv.org/rss/{category}")
        
        for entry in feed.entries[:5]:  # –ë–µ—Ä–µ–º 5 —Å—Ç–∞—Ç–µ–π
            article_id = entry.id.split('/')[-1]
            title = entry.title
            abstract = entry.summary
            
            # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            summary = summarizer(abstract, max_length=150, min_length=30)[0]['summary_text']
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            tags = classify_text(f"{title} {abstract}", tokenizer, classifier)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
            save_article({
                'id': article_id,
                'title': title,
                'summary': summary,
                'tags': tags
            })
            
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ: {title} | –¢–µ–≥–∏: {', '.join(tags)}")

# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥
def scheduled_parsing():
    summarizer, tokenizer, classifier = init_models()
    
    while True:
        try:
            parse_arxiv(summarizer, tokenizer, classifier)
            threading.Event().wait(PARSE_INTERVAL)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            threading.Event().wait(60)

# Telegram –±–æ—Ç
def start_bot():
    app = Application.builder().token(BOT_TOKEN).build()
    
    # –ö–æ–º–∞–Ω–¥–∞ /start
    async def start(update: Update, context):
        await update.message.reply_text(
            "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å arXiv.\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π /tags —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å —Ç–µ–≥ –∏ —É–≤–∏–¥–µ—Ç—å —Å—Ç–∞—Ç—å–∏."
        )
    
    # –ö–æ–º–∞–Ω–¥–∞ /tags
    async def show_tags(update: Update, context):
        keyboard = [[InlineKeyboardButton(tag, callback_data=tag)] for tag in ARXIV_CATEGORIES]
        await update.message.reply_text('–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ–≥:', reply_markup=InlineKeyboardMarkup(keyboard))
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —Ç–µ–≥–∞
    async def handle_tag(update: Update, context):
        query = update.callback_query
        await query.answer()
        
        articles = get_articles_by_tag(query.data)
        response = f"üìö –°—Ç–∞—Ç—å–∏ —Å —Ç–µ–≥–æ–º {query.data}:\n\n"
        
        for i, (art_id, title, summary) in enumerate(articles, 1):
            response += f"{i}. {title}\n{summary}\n\n"
        
        await query.edit_message_text(response)
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("tags", show_tags))
    app.add_handler(CallbackQueryHandler(handle_tag))
    
    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    app.run_polling()

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
if __name__ == '__main__':
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
    init_db()
    
    # –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –≤ —Ñ–æ–Ω–µ
    threading.Thread(target=scheduled_parsing, daemon=True).start()
    
    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    start_bot()