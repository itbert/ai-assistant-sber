Заголовок,Время публикации,Описание,Ссылка
Baidu ERNIE X1 and 4.5 Turbo boast high performance at low cost,"April 25, 2025","AI News is part of the TechForge Publications series TechForge Artificial Intelligence Ryan Daws April 25, 2025 Share this story: Tags: Categories:: Baiduhas unveiled ERNIE X1 Turbo and 4.5 Turbo, two fast models that boast impressive performance alongside dramatic cost reductions. Developed as enhancements to the existing ERNIE X1 and 4.5 models, both new Turbo versions highlight multimodal processing, robust reasoning skills, and aggressive pricing strategies designed to capture developer interest and marketshare. Positioned as a deep-thinking reasoning model, ERNIE X1 Turbo tackles complex tasks requiring sophisticated understanding. It enters a competitive field, claiming superior performance in some benchmarks against rivals likeDeepSeek R1, V3, and OpenAI o1: Key to X1 Turbo’s enhanced capabilities is an advanced “chain of thought” process, enabling more structured and logical problem-solving. Furthermore, ERNIE X1 Turbo boasts improved multimodal functions – the ability to understand and process information beyond just text, potentially including images or other data types – alongside refined tool utilisation abilities. This makes it particularly well-suited for nuanced applications such as literary creation, complex logical reasoning challenges, code generation, and intricate instruction following. ERNIE X1 Turbo achieves this performance while undercutting competitor pricing. Input token costs start at $0.14 per million tokens, with output tokens priced at $0.55 per million. This pricing structure is approximately 25% ofDeepSeek R1. Sharing the spotlight is ERNIE 4.5 Turbo, which focuses on delivering upgraded multimodal features and significantly faster response times compared to its non-Turbo counterpart. The emphasis here is on providing a versatile, responsive AI experience while slashing operational costs. The model achieves an 80% price reduction compared to the original ERNIE 4.5 with input set at $0.11 per million tokens and output at $0.44 per million tokens. This represents roughly 40% of the cost of the latest version ofDeepSeek V3, again highlighting a deliberate strategy to attract users through cost-effectiveness. Performance benchmarks further bolster its credentials. In multiple tests evaluating both multimodal and text capabilities, Baidu ERNIE 4.5 Turbo outperforms OpenAI’s highly-regarded GPT-4o model. In multimodal capability assessments, ERNIE 4.5 Turbo achieved an average score of 77.68 to surpass GPT-4o’s score of 72.76 in the same tests. While benchmark results always require careful interpretation, this suggests ERNIE 4.5 Turbo is a serious contender for tasks involving an integrated understanding of different data types. The launch of ERNIE X1 Turbo and 4.5 Turbo signifies a growing trend in the AI sector: the democratisation of high-end capabilities. While foundational models continue to push the boundaries of performance, there is increasing demand for models that balance power with accessibility and affordability. By lowering the price points for models with sophisticated reasoning and multimodal features, the Baidu ERNIE Turbo series could enable a wider range of developers and businesses to integrate advanced AI into their applications. This competitive pricing puts pressure on established players likeOpenAIandAnthropic, as well as emerging competitors like DeepSeek, potentially leading to further price adjustments across the market. (Image Credit:Alpha PhotounderCC BY-NC 2.0license) See also:China’s MCP adoption: AI assistants that actually do things Want to learn more about AI and big data from industry leaders?Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Ryan Daws Senior Editor April 25, 2025 April 24, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/baidu-ernie-x1-and-4-5-turbo-high-performance-low-cost/
How AI infrastructure is changing Solana’s price trends,"April 25, 2025","AI News is part of the TechForge Publications series TechForge Sponsored Content Shawna Rowe April 25, 2025 Share this story: Tags: Categories:: As of April 2025,Solana pricetrends – hovering near $141 – are attracting renewed attention from investors and developers. While market fluctuations are typical in the cryptocurrency space, Solana’s price movements are tied increasingly to its evolving role as an infrastructure layer for artificial intelligence. Solana’s technical architecture can handle high-speed, low-cost transactions, thus making it an appealing foundation for AI developers who build real-time, decentralised applications. The convergence of blockchain and AI is influencing the network’s utility and market valuation, creating new narratives around the future of scalable, intelligent systems. One appeal of Solana is its extremely agile performance. The network can theoretically support over 65,000 transactions per second (TPS), with real-world numbers averaging 3,000 to 4,500 TPS. This is possible because of the unique proof-of-history (PoH) mechanism, which timestamps transactions to enhance the validation process. Considering the consistently low transaction costs – an average of $0.036 per transaction – Solana offers an ecosystem for computation-dominated AI operations. Blockchain technology, at this level, allows large-scale interactions without latency or high costs. While the price of Solana generally mirrors the prevailing mood of the market, analysts have noted correlation between price movements and AI developments on the network. For example, the launch of AI-focused projects and integrations has caused price increases. Solana is emerging as a go-to hosting platform for AI-powered decentralised applications. Some of the more prominent projects using the network for AI include: These projects rely on the throughput of Solana to maintain real-time inference, moving large data streams, and executing microtransactions. Unlike many chains, Solana’s architecture accommodates AI applications that need to access the blockchain directly and quickly – without losing on cost. Most required micropayments in Solana for data, model updates, or compute payments, need to be executed frequently, via decentralisation. Through its fee structure, where the cost per interaction (transaction) is roughly $0.036, Solana guarantees low interaction costs. This capability fosters concepts like token-incentivised federated learning, autonomous model marketplace architectures, and on-the-go autonomous services – which each depend on micro-interactions unfeasible on slower, more-pricey blockchains. Blockchain analytics show a surge in transaction activity in Solana associated withAI tools and servicesimplementations. The network is capable of thousands of transactions per second; an increasing proportion of those include AI-related functions. The number of active daily addresses on Solana has increased in parallel, due to growing activity from developers working with AI, machine learning infrastructure, predictive analytics and real-time automation systems. The numbers show the demand for blockchain-associated workloads and development related to artificial intelligence is rising, which may help bolster Solana’s longer-term prospects and shape market sentiment – as evidenced by fluctuations in the price of Solana. Solana’s contributions to artificial intelligence do not stop at decentralised apps. The network also uses AI for its internal processes, with the Solana Foundation developing ML models for validator clustering and network optimisation. Using traffic patterns and predicting possible blockages,algorithmshelp maintain the lower latency Solana is known for, even at busy times. This increases the network’s resilience for applications like live dashboards powered by AI or data processing systems on the blockchain. Throughout last year, there was a great deal of venture capital investment in Solana-based projects that used AI add-ons. Important, funded projects include: The funding helps build infrastructure for functions that drive on-chain activity – like AI model training, decentralised inference, data lineage and provenance, and more. Transitions in Solana’s infrastructural, AI-transformable blockchain capabilities are changing the ecosystem’s scope. The architecture provides for services that demand speed, scalability and cost-effectiveness. Its features will help create next-generation, AI-dedicated, decentralised systems; effectively becoming serverless AI. With increasing adoption of AI models with blockchain implementation, Solana is set to become a foundation where information, logic, and algorithms co-exist. The ongoing development of advanced intelligent infrastructure on the Solana network provides an opportunity to redefine perceptions of changes to Solana’s price. (Image source: Unsplash) Shawna Rowe Okcoin Europe Limited and Binance Holdings Ltd April 25, 2025 April 24, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/how-ai-infrastructure-is-changing-solanas-price-trends/
Alarming rise in AI-powered scams: Microsoft reveals $4 Billion in thwarted fraud,"April 24, 2025","AI News is part of the TechForge Publications series TechForge Applications Dashveenjit Kaur April 24, 2025 Share this story: Tags: Categories:: AI-powered scams are evolving rapidly as cybercriminals use new technologies to target victims, according to Microsoft’s latestCyber Signals report. Over the past year, the tech giant says it has prevented $4 billion in fraud attempts, blocking approximately 1.6 million bot sign-up attempts every hour – showing the scale of this growing threat. The ninth edition of Microsoft’s Cyber Signals report, titled “AI-powered deception: Emerging fraud threats and countermeasures,” reveals how artificial intelligence has lowered the technical barriers for cybercriminals, enabling even low-skilled actors to generate sophisticated scams with minimal effort. What previously took scammers days or weeks to create can now be accomplished in minutes. The democratisation of fraud capabilities represents a shift in the criminal landscape that affects consumers and businesses worldwide. Microsoft’s report highlights how AI tools can now scan and scrape the web for company information, helping cybercriminals build detailed profiles of potential targets for highly-convincing social engineering attacks. Bad actors can lure victims into complex fraud schemes using fake AI-enhanced product reviews and AI-generated storefronts, which come complete with fabricated business histories and customer testimonials. According to Kelly Bissell, Corporate Vice President of Anti-Fraud and Product Abuse at Microsoft Security, the threat numbers continue to increase. “Cybercrime is a trillion-dollar problem, and it’s been going up every year for the past 30 years,” per the report. “I think we have an opportunity today to adopt AI faster so we can detect and close the gap of exposure quickly. Now we have AI that can make a difference at scale and help us build security and fraud protections into our products much faster.” The Microsoft anti-fraud team reports that AI-powered fraud attacks happen globally, with significant activity originating from China and Europe – particularly Germany, due to its status as one of the largest e-commerce markets in the European Union. The report notes that the larger a digital marketplace is, the more likely a proportional degree of attempted fraud will occur. Two particularly concerning areas of AI-enhanced fraud include e-commerce and job recruitment scams.In the ecommerce space, fraudulent websites can now be created in minutes using AI tools with minimal technical knowledge. Sites often mimic legitimate businesses, using AI-generated product descriptions, images, and customer reviews to fool consumers into believing they’re interacting with genuine merchants. Adding another layer of deception, AI-powered customer service chatbots can interact convincingly with customers, delay chargebacks by stalling with scripted excuses, and manipulate complaints with AI-generated responses that make scam sites appear professional. Job seekers are equally at risk. According to the report, generative AI has made it significantly easier for scammers to create fake listings on various employment platforms. Criminals generate fake profiles with stolen credentials, fake job postings with auto-generated descriptions, and AI-powered email campaigns to phish job seekers. AI-powered interviews and automated emails enhance the credibility of these scams, making them harder to identify. “Fraudsters often ask for personal information, like resumes or even bank account details, under the guise of verifying the applicant’s information,” the report says. Red flags include unsolicited job offers, requests for payment and communication through informal platforms like text messages or WhatsApp. To combat emerging threats, Microsoft says it has implemented a multi-pronged approach across its products and services. Microsoft Defender for Cloud provides threat protection for Azure resources, while Microsoft Edge, like many browsers, features website typo protection and domain impersonation protection. Edge is noted by the Microsoft report as using deep learning technology to help users avoid fraudulent websites. The company has also enhanced Windows Quick Assist with warning messages to alert users about possible tech support scams before they grant access to someone claiming to be from IT support. Microsoft now blocks an average of 4,415 suspicious Quick Assist connection attempts daily. Microsoft has also introduced a new fraud prevention policy as part of its Secure Future Initiative (SFI). As of January 2025, Microsoft product teams must perform fraud prevention assessments and implement fraud controls as part of their design process, ensuring products are “fraud-resistant by design.” As AI-powered scams continue to evolve, consumer awareness remains important. Microsoft advises users to be cautious of urgency tactics, verify website legitimacy before making purchases, and never provide personal or financial information to unverified sources. For enterprises, implementing multi-factor authentication anddeploying deepfake-detectionalgorithms can help mitigate risk. See also:Wozniak warns AI will power next-gen scams Want to learn more about AI and big data from industry leaders? Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Dashveenjit Kaur Journalist April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/alarming-rise-in-ai-powered-scams-microsoft-reveals-4-billion-in-thwarted-fraud/
RAGEN: AI framework tackles LLM agent instability,"April 24, 2025","AI News is part of the TechForge Publications series TechForge Applications Ryan Daws April 24, 2025 Share this story: Tags: Categories:: Researchers have introduced RAGEN, an AI framework designed to counter LLM agent instability when handling complex situations. Training these AI agents presents significant hurdles, particularly when decisions span multiple steps and involve unpredictable feedback from the environment. While reinforcement learning (RL) has shown promise in static tasks like solving maths problems or generating code, its application to dynamic, multi-turn agent training has been less explored. Addressing this gap, a collaborative team from institutions includingNorthwestern University,Stanford University,Microsoft, andNew York Universityhas proposed StarPO (State-Thinking-Actions-Reward Policy Optimisation). StarPO offers a generalised approach for training agents at the trajectory level (i.e. it optimises the entire sequence of interactions, not just individual actions.) Accompanying this is RAGEN, a modular system built to implement StarPO. This enables the training and evaluation of LLM agents, particularly focusing on their reasoning capabilities under RL. RAGEN provides the necessary infrastructure for rollouts, reward assignment, and optimisation within multi-turn, stochastic (randomly determined) environments. To isolate the core learning challenges from confounding factors like extensive pre-existing knowledge or task-specific engineering, the researchers tested LLMs using RAGEN in three deliberately minimalistic, controllable symbolic gaming environments: These environments allow for clear analysis of how agents learn decision-making policies purely through interaction. The study yielded three significant findings concerning the training of self-evolving LLM agents: The ‘Echo Trap’ and the need for stability A recurring problem observed during multi-turn RL training was dubbed the “Echo Trap”. Agents would initially improve but then suffer performance collapse, overfitting to locally rewarded reasoning patterns. This was marked by collapsing reward variance, falling entropy (a measure of randomness/exploration), and sudden spikes in gradients (indicating training instability). Early signs included drops in reward standard deviation and output entropy. To combat this, the team developed StarPO-S, a stabilised version of the framework. StarPO-S incorporates: StarPO-S consistently delayed collapse and improved final task performance compared to vanilla StarPO. Rollout quality is crucial The characteristics of the ‘rollouts’ (simulated interaction trajectories used for training) significantly impact learning. Key factors identified include: Maintaining freshness, alongside appropriate action budgets and task diversity, is key for stable training. Reasoning requires careful reward design Simply prompting models to ‘think’ doesn’t guarantee meaningful reasoning emerges, especially in multi-turn tasks. The study found: This suggests that standard trajectory-level rewards (often sparse and outcome-based) are insufficient. “Without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge[s] through multi-turn RL.” The researchers propose that future work should explore rewards that explicitly evaluate the quality of intermediate reasoning steps, perhaps using format-based penalties or rewarding explanation quality, rather than just final outcomes. The RAGEN system and StarPO framework represent a step towards training LLM agents that can reason and adapt through interaction in complex, unpredictable environments. This research highlights the unique stability challenges posed by multi-turn RL and offers concrete strategies – like StarPO-S’s filtering and stabilisation techniques – to mitigate them. It also underscores the critical role of rollout generation strategies and the need for more sophisticated reward mechanisms to cultivate genuine reasoning, rather than superficial strategies or hallucinations. Why does your RL training always collapse?In our new paper of RAGEN, we explore what breaks when you train LLM *Agents* with multi-turn reinforcement learning—and possibly how to fix it.https://t.co/z0U0612HWThttps://t.co/4DUfaees481/pic.twitter.com/Oy6ilkgimd While acknowledging limitations – including the need to test on larger models and optimise for domains without easily verifiable rewards – the work opens “a scalable and principled path for building AI systems” in areas demanding complex interaction and verifiable outcomes, such as theorem proving, software engineering, and scientific discovery. (Image byGerd Altmann) See also:How does AI judge? Anthropic studies the values of Claude Want to learn more about AI and big data from industry leaders?Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Ryan Daws Senior Editor April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/ragen-ai-framework-tackles-llm-agent-instability/
Coalition opposes OpenAI shift from nonprofit roots,"April 24, 2025","AI News is part of the TechForge Publications series TechForge AGI Ryan Daws April 24, 2025 Share this story: Tags: Categories:: A coalition of experts, including former OpenAI employees, has voiced strong opposition to the company’s shift away from its nonprofit roots. In anopen letteraddressed to the Attorneys General of California and Delaware, the group – which also includes legal experts, corporate governance specialists, AI researchers, and nonprofit representatives – argues that the proposed changes fundamentally threaten OpenAI’s original charitable mission. OpenAI was founded with a unique structure. Its core purpose, enshrined in its Articles of Incorporation, is “to ensure that artificial general intelligence benefits all of humanity” rather than serving “the private gain of any person.” The letter’s signatories contend that the planned restructuring – transforming the current for-profit subsidiary (OpenAI-profit) controlled by the original nonprofit entity (OpenAI-nonprofit) into a Delaware public benefit corporation (PBC) – would dismantle crucial governance safeguards. This shift, the signatories argue, would transfer ultimate control over the development and deployment of potentially transformative Artificial General Intelligence (AGI) from a charity focused on humanity’s benefit to a for-profit enterprise accountable to shareholders. OpenAI defines AGI as “highly autonomous systems that outperform humans at most economically valuable work”. While acknowledging AGI’s potential to “elevate humanity,” OpenAI’s leadership has also warned of “serious risk of misuse, drastic accidents, and societal disruption.” Co-founder Sam Altman and others have even signed statements equating mitigating AGI extinction risks with preventing pandemics and nuclear war. The company’s founders – including Altman, Elon Musk, and Greg Brockman – were initially concerned about AGI being developed by purely commercial entities like Google. They established OpenAI as a nonprofit specifically “unconstrained by a need to generate financial return”. As Altman stated in 2017, “The only people we want to be accountable to is humanity as a whole.” Even when OpenAI introduced a “capped-profit” subsidiary in 2019 to attract necessary investment, it emphasised that the nonprofit parent would retain control and that the mission remained paramount. Key safeguards included: Altman himself testified to Congress in 2023 that this “unusual structure” “ensures it remains focused on [its] long-term mission.” The critics argue the move to a PBC structure would jeopardise these safeguards: OpenAI has publicly cited competitive pressures (i.e. attracting investment and talent against rivals with conventional equity structures) as reasons for the change. However, the letter counters that competitive advantage isn’t the charitable purpose of OpenAI and that its unique nonprofit structure was designed to impose certain competitive costs in favour of safety and public benefit. “Obtaining a competitive advantage by abandoning the very governance safeguards designed to ensure OpenAI remains true to its mission is unlikely to, on balance, advance the mission,” the letter states. The authors also question why OpenAI abandoning nonprofit control is necessary merely to simplify the capital structure, suggesting the core issue is the subordination of investor interests to the mission. They argue that while the nonprofit board can consider investor interests if it serves the mission, the restructuring appears aimed at allowing these interests to prevail at the expense of the mission. Many of these arguments have also been pushed by Elon Musk in his legal action against OpenAI. Earlier this month, OpenAIcounter-sued Muskfor allegedly orchestrating a “relentless” and “malicious” campaign designed to “take down OpenAI” after he left the company years ago and started rival AI firm xAI. The signatories of the open letter urge intervention, demanding answers from OpenAI about how the restructuring away from a nonprofit serves its mission and why safeguards previously deemed essential are now obstacles. Furthemore, the signatories request a halt to the restructuring, preservation of nonprofit control and other safeguards, and measures to ensure the board’s independence and ability to oversee management effectively in line with the charitable purpose. “The proposed restructuring would eliminate essential safeguards, effectively handing control of, and profits from, what could be the most powerful technology ever created to a for-profit entity with legal duties to prioritise shareholder returns,” the signatories conclude. See also:How does AI judge? Anthropic studies the values of Claude Want to learn more about AI and big data from industry leaders?Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Ryan Daws Senior Editor April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/coalition-opposes-openai-shift-from-nonprofit-roots/
Reigniting the European digital economy’s €200bn AI ambitions,"April 24, 2025","AI News is part of the TechForge Publications series TechForge Quantum Computing GITEX EUROPE April 24, 2025 Share this story: Tags: Categories:: There is a sense of urgency in Europe to re-imagine the status quo and reshape technology infrastructures. Timed to harness Europe’s innovative push comesGITEX EUROPE x Ai Everything(21-23 May, Messe Berlin). The world’s third largest economy and host nation for GITEX EUROPE x Ai Everything, Germany’s role as the European economic and technology leader is confirmed as its ICT sector is projected to reach €232.8bn in 2025 (Statista). GITEX EUROPE x Ai Everything is Europe’s largest tech, startup and digital investment event, and is organised by KAOUN International. It’s hosted in partnership with the Berlin Senate Department for Economics, Energy and Public Enterprises, Germany’s Federal Ministry for Economic Affairs and Climate Action, Berlin Partner for Business and Technology, and the European Innovation Council (EIC). The first GITEX EUROPE brings together over 1,400 tech enterprises, startups and SMEs, and platinum sponsors AWS and IBM. Also in sponsorship roles are Cisco, Cloudflare, Dell, Fortinet, Lenovo, NTT, Nutanix, Nvidia, Opswat, and SAP. GITEX EUROPE x Ai Everything will comprise of tech companies from over 100 countries and 34 European states, including tech pavilions from India, Italy, Morocco, Netherlands, Poland, Serbia, South Korea, UK, and the UAE. Trixie LohMirmand, CEO of KAOUN International, organiser of GITEX worldwide, said: “There is a sense of urgency and unity in Europe to assert its digital sovereignty and leadership as a global innovation force. The region is paving its way as a centre-stage where AI, quantum and deep tech will be debated, developed, and scaled.” Organisers state there will be over 500 speakers, debating a range of issues including AI and quantum, cloud, and data sovereignty. Already confirmed are Geoffrey Hinton, Physics Nobel Laureate (2024); Kai Wegner, Mayor of Berlin; H.E. Jelena Begović, Serbian Minister of Science, Technological Development and Innovation; António Henriques, CEO, Bison Bank; Jager McConnell, CEO, Crunchbase; Mark Surman, President, Mozilla; and Sandro Gianella, Head of Europe & Middle East Policy & Partnerships, OpenAI. Europe is focusing on cross-sector AI uses, new investments and international partnerships. Ai Everything Europe, the event’s AI showcase and conference, brings together AI architects, startups and investors to explore AI ecosystems. Topics presented on stage range from EuroStack ambitions to implications of agentic AI, with speakers including Martin Kon, President and COO, Cohere; Daniel Verten, Strategy Partner, Synthesia; and Professor Dr. Antonio Krueger, CEO of German Research Centre for Artificial Intelligence. On the show-floor, attendees will be able to experience Brazil’s Ubivis’s smart factory technology, powered by IoT and digital twins, and Hexis’s AI-driven nutrition plans that are trusted by 500+ Olympic and elite athletes. With nearly €7 billion in quantum investment, Europe is pushing for quantum leadership by 2030. GITEX Quantum Expo (GQX) (in partnership with IBM and QuIC) covers quantum research and cross-industry impact with showcases and conferences. Speakers include Mira Wolf-Bauwens, Responsible Quantum Computing Lead, IBM Research, Switzerland; Joachim Mnich, Director of Research & Computing, CERN, Switzerland; Neil Abroug, Head of the French National Quantum Strategy, INRIA; and Jan Goetz, CEO & Co-Founder, IQM Quantum Computers, Finland. With cloud breaches doubling in number and AI-driven attacks, threat response and cyber resilience are core focuses at the event. Fortinet, CrowdStrike, Kaspersky, Knowbe4, and Proofpoint will join other cybersecurity companies exhibiting at GITEX Cyber Valley. They’ll be alongside law enforcement leaders, global CISOs, and policymakers on stage, including Brig. Gen. Dr. Volker Pötzsch, Chief of Division Cyber/IT & AI, Federal Ministry of Defence, Germany; H.E. Dr. Mohamed Al-Kuwaiti, Head of Cybersecurity, UAE Government; Miguel De Bruycker, Managing Director General, Centre for Cybersecurity Belgium; and Ugo Vignolo Lutati, Group CISO, Prada Group. GITEX Green Impact connects innovators and investors with over 100 startups and investors exploring how green hydrogen, bio-energy, and next-gen energy storage are moving from R&D to deployment. Key speakers so far confirmed are Gavin Towler, Chief Scientist for Sustainability Technologies & CTO, Honeywell; Julie Kitcher, Chief Sustainability Officer, Airbus; Lisa Reehten, Managing Director, Bosch Climate Solutions; Massimo Falcioni, Chief Competitiveness Officer, Abu Dhabi Investment Office; and Mounir Benaija, CTO – EV & Charging Infrastructure, TotalEnergies. GITEX EUROPE x Ai Everything hosts North Star Europe, the local version of the world’s largest startup event, Expand North Star. North Star Europe gathers over 750 startups and 20 global unicorns, among them reMarkable, TransferMate, Solarisbank AG, Bolt, Flix, and Glovo. The event features a curated collection of earlys and growth-stage startups from Belgium, France, Hungary, Italy, Morocco, Portugal, Netherlands, Switzerland, Serbia, UK, and UAE. Among the startups, Neurocast.ai (Netherlands) is advancing AI-powered neurotech for Alzheimer’s research; CloudBees (Switzerland) is the delivery unicorn backed by Goldman Sachs, HSBC, and Lightspeed; and Semiqon (Finland), the world’s first CMOS transistor with the ability to perform in cryogenic conditions. More than 600 investors with $1tn assets under management will be scouting for new opportunities, including Germany’s Earlybird VC, Austria’s SpeedInvest, Switzerland’s B2Venture, Estonia’s Startup Wise Guys, and the US’s SOSV. GITEX ScaleX launches as a first-of-its-kind growth platform for scale-ups and late-stage companies, in partnership with AWS. With SMEs making up 99% of European businesses, GITEX SMEDEX connects SMEs with international trade networks and investors, for funding, legal advice, and market access to scale globally. Backed by EISMEA and ICC Digital Standards Initiative, the event features SME ecosystem leaders advising from the stage, including Milena Stoycheva, Chairperson of Board of Innovation, Ministry of Innovation and Growth, Bulgaria; and Oliver Grün, President, European Digital SME Alliance and BITMi. GITEX EUROPE is part of the GITEX global network tech and startup events, taking place in Germany, Morocco, Nigeria, Singapore, Thailand, and the UAE. For more information, please visit:www.gitex-europe.com.  GITEX EUROPE GITEX EUROPE April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/gitex-europe-2025/
How does AI judge? Anthropic studies the values of Claude,"April 23, 2025","AI News is part of the TechForge Publications series TechForge Artificial Intelligence Ryan Daws April 23, 2025 Share this story: Tags: Categories:: AI models like Anthropic Claude are increasingly asked not just for factual recall, but for guidance involving complex human values. Whether it’s parenting advice, workplace conflict resolution, or help drafting an apology, the AI’s response inherently reflects a set of underlying principles. But how can we truly understand which values an AI expresses when interacting with millions of users? In a research paper, the Societal Impacts team at Anthropic details a privacy-preserving methodology designed to observe and categorise the values Claude exhibits “in the wild.” This offers a glimpse into how AI alignment efforts translate into real-world behaviour. The core challenge lies in the nature of modern AI. These aren’t simple programs following rigid rules; their decision-making processes are often opaque. Anthropic says it explicitly aims to instil certain principles in Claude, striving to make it “helpful, honest, and harmless.” This is achieved through techniques like Constitutional AI and character training, where preferred behaviours are defined and reinforced. However, the company acknowledges the uncertainty. “As with any aspect of AI training, we can’t be certain that the model will stick to our preferred values,” the research states. “What we need is a way of rigorously observing the values of an AI model as it responds to users ‘in the wild’ […] How rigidly does it stick to the values? How much are the values it expresses influenced by the particular context of the conversation? Did all our training actually work?” To answer these questions, Anthropic developed a sophisticated system that analyses anonymised user conversations. This system removes personally identifiable information before using language models to summarise interactions and extract the values being expressed by Claude. The process allows researchers to build a high-level taxonomy of these values without compromising user privacy. The study analysed a substantial dataset: 700,000 anonymised conversations from Claude.ai Free and Pro users over one week in February 2025, predominantly involving the Claude 3.5 Sonnet model. After filtering out purely factual or non-value-laden exchanges, 308,210 conversations (approximately 44% of the total) remained for in-depth value analysis. The analysis revealed a hierarchical structure of values expressed by Claude. Five high-level categories emerged, ordered by prevalence: These top-level categories branched into more specific subcategories like “professional and technical excellence” or “critical thinking.” At the most granular level, frequently observed values included “professionalism,” “clarity,” and “transparency” – fitting for an AI assistant. Critically, the research suggests Anthropic’s alignment efforts are broadly successful. The expressed values often map well onto the “helpful, honest, and harmless” objectives. For instance, “user enablement” aligns with helpfulness, “epistemic humility” with honesty, and values like “patient wellbeing” (when relevant) with harmlessness. However, the picture isn’t uniformly positive. The analysis identified rare instances where Claude expressed values starkly opposed to its training, such as “dominance” and “amorality.” Anthropic suggests a likely cause: “The most likely explanation is that the conversations that were included in these clusters were from jailbreaks, where users have used special techniques to bypass the usual guardrails that govern the model’s behavior.” Far from being solely a concern, this finding highlights a potential benefit: the value-observation method could serve as an early warning system for detecting attempts to misuse the AI. The study also confirmed that, much like humans, Claude adapts its value expression based on the situation. When users sought advice on romantic relationships, values like “healthy boundaries” and “mutual respect” were disproportionately emphasised. When asked to analyse controversial history, “historical accuracy” came strongly to the fore. This demonstrates a level of contextual sophistication beyond what static, pre-deployment tests might reveal. Furthermore, Claude’s interaction with user-expressed values proved multifaceted: Anthropic is candid about the method’s limitations. Defining and categorising “values” is inherently complex and potentially subjective. Using Claude itself to power the categorisation might introduce bias towards its own operational principles. This method is designed formonitoring AI behaviourpost-deployment, requiring substantial real-world data and cannot replace pre-deployment evaluations. However, this is also a strength, enabling the detection of issues – including sophisticated jailbreaks – that only manifest during live interactions. The research concludes that understanding the values AI models express is fundamental to the goal of AI alignment. “AI models will inevitably have to make value judgments,” the paper states. “If we want those judgments to be congruent with our own values […] then we need to have ways of testing which values a model expresses in the real world.” This work provides a powerful, data-driven approach to achieving that understanding. Anthropic has also released an open dataset derived from the study, allowing other researchers to further explore AI values in practice. This transparency marks a vital step in collectively navigating the ethical landscape of sophisticated AI. We’ve made the dataset of Claude’s expressed values open for anyone to download and explore for themselves.Download the data:https://t.co/rxwPsq6hXf See also:Google introduces AI reasoning control in Gemini 2.5 Flash Want to learn more about AI and big data from industry leaders?Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Ryan Daws Senior Editor April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/how-does-ai-judge-anthropic-studies-values-of-claude/
China’s MCP adoption: AI assistants that actually do things,"April 23, 2025","AI News is part of the TechForge Publications series TechForge Alibaba Dashveenjit Kaur April 23, 2025 Share this story: Tags: Categories:: China’s tech companies will drive adoption of the MCP (Model Context Protocol) standard that transforms AI assistants from simple chatbots into powerful digital helpers. MCP works like a universal connector that lets AI assistants interact directly with favourite apps and services – enabling them to make payments, book appointments, check maps, and access information on different platforms on users’ behalves. As reported by theSouth China Morning Post, companies like Ant Group, Alibaba Cloud, and Baidu are deploying MCP-based services and positioning AI agents as the next step, after chatbots and large language models. But will China’s MCP adoption truly transform the AI landscape, or is it simply another step in the technology’s evolution? The Model Context Protocol was initially introduced by Anthropic in November 2024, at the time described as a standard that connects AI agents “to the systems where data lives, including content repositories, business tools and development environments.” MCP serves as what Ant Group calls a “USB-C port for AI applications” – a universal connector allowing AI agents to integrate with multiple systems. The standardisation is particularly significant for AI agents likeButterfly Effect’s Manus, which are designed to autonomously perform tasks by creating plans consisting of specific subtasks using available resources. Unlike traditional chatbots that just respond to queries, AI agents can actively interact with different systems, collect feedback, and incorporate that feedback into new actions. China’s MCP adoption by tech leaders highlights the importance placed on AI agents as the next evolution in artificial intelligence: China’s MCP adoption signals a shift in focus from large language models and chatbots to more capable AI agents. As Red Xiao Hong, founder and CEO of Butterfly Effect, described, an AI agent is “more like a human being” compared to how chatbots perform. The agents not only respond to questions but “interact with the environment, collect feedback and use the feedback as a new prompt.” This distinction is held to be important by companies driving progress in AI. While chatbots and LLMs can generate text and respond to queries, AI agents can take actions on multiple platforms and services. They represent an advance from the limited capabilities of conventional AI applications toward autonomous systems capable of completing more complex tasks with less human intervention. The rapid embrace of MCP by Chinese tech companies suggests they view AI agents as a new avenue for innovation and commercial opportunity that go beyond what’s possible with existing chatbots and language models. China’s MCP adoption could position its tech companies at the forefront of practical AI implementation. By creating standardised ways for AI agents to interact with services, Chinese companies are building ecosystems where AI could deliver more comprehensive experiences. Despite the developments in China’s MCP adoption, several factors may influence the standard’s longer-term impact: China’s MCP adoption represents a strategic bet on AI agents as the next evolution in artificial intelligence. If successful, it could accelerate the practical implementation of AI in everyday applications, potentially transforming how users interact with digital services. As Red Xiao Hong noted, AI agents are designed to interact with their environment in ways that more closely resemble human behaviour than traditional AI applications. The capacity for interaction and adaptation could be what finally bridges the gap between narrow AI tools and the more generalised assistants that tech companies have long promised. See also:Manus AI agent: breakthrough in China’s agentic AI Want to learn more about AI and big data from industry leaders? Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Dashveenjit Kaur Journalist April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/chinas-mcp-adoption-ai-assistants-that-actually-do-things/
AI memory demand propels SK Hynix to historic DRAM market leadership,"April 23, 2025","AI News is part of the TechForge Publications series TechForge Blockchain Dashveenjit Kaur April 23, 2025 Share this story: Tags: Categories:: AI memory demand has catapulted SK Hynix to a top position in the global DRAM market, overtaking longtime leader Samsung for the first time. According to Counterpoint Researchdata, SK Hynix captured 36% of the DRAM market in Q1 2025, compared to Samsung’s 34% share. The company’s achievement ends Samsung’s three-decade dominance in DRAM manufacturing and comes shortly after SK Hynix’s operating profit passed Samsung’s in Q4 2024. The company’s strategic focus on high-bandwidth memory (HBM) chips, essential components for artificial intelligence applications, has proven to be the decisive factor in the market shift. “The is a milestone for SK Hynix which is successfully delivering on DRAM to a market that continues to show unfettered demand for HBM memory,” said Jeongku Choi, senior analyst at Counterpoint Research. “The manufacturing of specialised HBM DRAM chips has been notoriously tricky and those that got it right early on have reaped dividends.” SK Hynix has taken the overall DRAM market lead and has established its dominance in the HBM sector, occupying 70% of this high-value market segment, according to Counterpoint Research. HBM chips, which stack multiple DRAM dies to dramatically increase data processing capabilities, have become fundamental components for training AI models. “It’s another wake-up call for Samsung,” said MS Hwang, research director at Counterpoint Research in Seoul, as quoted byBloomberg. Hwang noted that SK Hynix’s leadership in HBM chips likely comprised a larger portion of the company’s operating income. The company is expected to report positive financial results on Thursday, with analysts projecting a 38% quarterly rise in sales and a 129% increase in operating profit for the March quarter, according toBloombergdata. The shift in market leadership reflects broader changes in the semiconductor industry as AI applications drive demand for specialised memory solutions. While traditional DRAM remains essential for computing devices, HBM chips that can handle the enormous data requirements of generative AI systems are becoming increasingly valuable. Market research firm TrendForce forecasts that SK Hynix will maintain its leadership position throughout 2025, coming to control over 50% of the HBM market in gigabit shipments. Samsung’s share is expected to decline to under 30%, while Micron Technology is said to gain ground to take close to 20% of the market. Counterpoint Research expects the overall DRAM market in Q2 2025 to maintain similar patterns across segment growth and vendor share, suggesting SK Hynix’s newfound leadership position may be sustainable in the near term. Despite the current AI memory demand boom, industry analysts identify several challenges on the horizon. “Right now the world is focused on the impact of tariffs, so the question is: what’s going to happen with HBM DRAM?” said MS Hwang. “At least in the short term, the segment is less likely to be affected by any trade shock as AI demand should remain strong. More significantly, the end product for HBM is AI servers, which – by definition – can be borderless.” However, longer-term risks remain significant. Counterpoint Research sees potential threats to HBM DRAM market growth “stemming from structural challenges brought on by trade shock that could trigger a recession or even a depression.” Morgan Stanley analysts, led by Shawn Kim, expressed similar sentiment in a note to investors cited byBloomberg: “The real tariff impact on memory resembles an iceberg, with most danger unseen below the surface and still approaching.” The analysts cautioned that earnings reports might be overshadowed by these larger macroeconomic forces. Interestingly, despite SK Hynix’s current advantage, Morgan Stanley still favours Samsung as their top pick in the memory sector. “It can better withstand a macro slowdown, is priced at trough multiples, has optionality of future growth via HBM, and is buying back shares every day,” analysts wrote. Samsung is scheduled to provide its complete financial statement with net income and divisional breakdowns on April 30, after reporting preliminary operating profit of 6.6 trillion won ($6 billion) on revenue of 79 trillion won earlier this month. The shift in competitive positioning between the two South Korean memory giants underscores how specialised AI components are reshaping the semiconductor industry. SK Hynix’s early and aggressive investment in HBM technology has paid off, though Samsung’s considerable resources ensure the rivalry will continue. For the broader technology ecosystem, the change in DRAM market leadership signals the growing importance of AI-specific hardware components. As data centres worldwide continue expanding to support increasingly-sophisticated AI models, AI memory demand should remain robust despite potential macroeconomic headwinds. (Image credit: SK Hynix) See also:Samsung aims to boost on-device AI with LPDDR5X DRAM Want to learn more about AI and big data from industry leaders? Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Dashveenjit Kaur Journalist April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/ai-memory-demand-propels-sk-hynix-to-historic-dram-market-leadership/
Google introduces AI reasoning control in Gemini 2.5 Flash,"April 23, 2025","AI News is part of the TechForge Publications series TechForge Blockchain Dashveenjit Kaur April 23, 2025 Share this story: Tags: Categories:: Google has introduced an AI reasoning control mechanism for itsGemini2.5 Flash model that allows developers to limit how much processing power the system expends on problem-solving. Released on April 17, this “thinking budget” feature responds to a growing industry challenge: advanced AI models frequently overanalyse straightforward queries, consuming unnecessary computational resources and driving up operational and environmental costs. While not revolutionary, the development represents a practical step toward addressing efficiency concerns that have emerged as reasoning capabilities become standard in commercial AI software. The new mechanism enables precise calibration of processing resources before generating responses, potentiallychanginghow organisations manage financial and environmental impacts of AI deployment. “The model overthinks,” acknowledges Tulsee Doshi, Director of Product Management at Gemini. “For simple prompts, the model does think more than it needs to.” The admission reveals the challenge facing advanced reasoning models – the equivalent of using industrial machinery to crack a walnut. The shift toward reasoning capabilities has created unintended consequences. Where traditional large language models primarily matched patterns from training data, newer iterations attempt to work through problems logically, step by step. While this approach yields better results for complex tasks, it introduces significant inefficiency when handling simpler queries. The financial implications of unchecked AI reasoning are substantial. According to Google’s technical documentation, when full reasoning is activated, generating outputs becomes approximately six times more expensive than standard processing. The cost multiplier creates a powerful incentive for fine-tuned control. Nathan Habib, an engineer at Hugging Face who studies reasoning models, describes the problem as endemic across the industry. “In the rush to show off smarter AI, companies are reaching for reasoning models like hammers even where there’s no nail in sight,” he explained toMIT Technology Review. The waste isn’t merely theoretical. Habib demonstrated how a leading reasoning model, when attempting to solve an organic chemistry problem, became trapped in a recursive loop, repeating “Wait, but…” hundreds of times – essentially experiencing a computational breakdown and consuming processing resources. Kate Olszewska, who evaluates Gemini models at DeepMind, confirmed Google’s systems sometimes experience similar issues, getting stuck in loops that drain computing power without improving response quality. Google’s AI reasoning control provides developers with a degree of precision. The system offers a flexible spectrum ranging from zero (minimal reasoning) to 24,576 tokens of “thinking budget” – the computational units representing the model’s internal processing. The granular approach allows for customised deployment based on specific use cases. Jack Rae, principal research scientist at DeepMind, says that defining optimal reasoning levels remains challenging: “It’s really hard to draw a boundary on, like, what’s the perfect task right now for thinking.” The introduction of AI reasoning control potentially signals a change in how artificial intelligence evolves. Since 2019, companies have pursued improvements by building larger models with more parameters and training data. Google’s approach suggests an alternative path focusing on efficiency rather than scale. “Scaling laws are being replaced,” says Habib, indicating that future advances may emerge from optimising reasoning processes rather than continuously expanding model size. The environmental implications are equally significant. As reasoning models proliferate, their energy consumption grows proportionally. Research indicates that inferencing – generating AI responses – now contributes more to the technology’s carbon footprint than the initial training process. Google’s reasoning control mechanism offers a potential mitigating factor for this concerning trend. Google isn’t operating in isolation. The “open weight” DeepSeek R1 model, which emerged earlier this year, demonstrated powerful reasoning capabilities at potentially lower costs, triggering market volatility that reportedly caused nearly a trillion-dollar stock market fluctuation. Unlike Google’s proprietary approach, DeepSeek makes its internal settings publicly available for developers to implement locally. Despite the competition, Google DeepMind’s chief technical officer Koray Kavukcuoglu maintains that proprietary models will maintain advantages in specialised domains requiring exceptional precision: “Coding, math, and finance are cases where there’s high expectation from the model to be very accurate, to be very precise, and to be able to understand really complex situations.” The development of AI reasoning control reflects an industry now confronting practical limitations beyond technical benchmarks. While companies continue to push reasoning capabilities forward, Google’s approach acknowledges a important reality: efficiency matters as much as raw performance in commercial applications. The feature also highlights tensions between technological advancement and sustainability concerns. Leaderboards tracking reasoning model performance show that single tasks can cost upwards of $200 to complete – raising questions about scaling such capabilities in production environments. By allowing developers to dial reasoning up or down based on actual need, Google addresses both financial and environmental aspects of AI deployment. “Reasoning is the key capability that builds up intelligence,” states Kavukcuoglu. “The moment the model starts thinking, the agency of the model has started.” The statement reveals both the promise and the challenge of reasoning models – their autonomy creates both opportunities and resource management challenges. For organisations deploying AI solutions, the ability to fine-tune reasoning budgets could democratise access to advanced capabilities while maintaining operational discipline. Google claims Gemini 2.5 Flash delivers “comparable metrics to other leading models for a fraction of the cost and size” – a value proposition strengthened by the ability to optimise reasoning resources for specific applications. The AI reasoning control feature has immediate practical applications. Developers building commercial applications can now make informed trade-offs between processing depth and operational costs. For simple applications like basic customer queries, minimal reasoning settings preserve resources while still using the model’s capabilities. For complex analysis requiring deep understanding, the full reasoning capacity remains available. Google’s reasoning ‘dial’ provides a mechanism for establishing cost certainty while maintaining performance standards. See also:Gemini 2.5: Google cooks up its ‘most intelligent’ AI model to date Want to learn more about AI and big data from industry leaders? Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere.  Dashveenjit Kaur Journalist April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/google-introduces-ai-reasoning-control-gemini-2-5-flash/
Huawei to begin mass shipments of Ascend 910C amid US curbs,"April 23, 2025","AI News is part of the TechForge Publications series TechForge Artificial Intelligence Muhammad Zulhusni April 23, 2025 Share this story: Tags: Categories:: Huawei is expected to begin large-scale shipments of the Ascend 910C AI chip as early as next month, according to people familiar with the matter. While limited quantities have already been delivered, mass deployment would mark an important step for Chinese firms seeking domestic alternatives to US-made semiconductors. The move comes at a time when Chinese developers face tighter restrictions on access to Nvidia hardware. The US government recently informed Nvidia that sales of its H20 AI chip to China require an export licence. That’s left developers in China looking for options that can support large-scale training and inference workloads. The Huawei Ascend 910C chip isn’t built on the most advanced process nodes, but it represents a workaround. The chip is essentially a dual-package version of the earlier 910B, with two processors to double the performance and memory. Sources familiar with the chip say it performs comparably to Nvidia’s H100. Rather than relying on cutting-edge manufacturing, Huawei has adopted a brute-force approach, combining multiple chips and high-speed optical interconnects to scale up performance. This approach is central to Huawei’s CloudMatrix 384 system, a full rack-scale AI platform for training large models. The CloudMatrix 384 features 384 Huawei Ascend 910C chips deployed in 16 racks comprising of 12 compute racks and four networking. Unlike copper-based systems, Huawei’s platform is uses optical interconnects, enabling high-bandwidth communication between components of the system. According to analysis from SemiAnalysis, the architecture includes 6,912 800G LPO optical transceivers to form an optical all-to-all mesh network. This allows Huawei’s system to deliver approximately 300 petaFLOPs of BF16 compute power – outpacing Nvidia’s GB200 NVL72 system, which reaches around 180 BF16 petaFLOPs. The CloudMatrix also claims advantages in higher memory bandwidth and capacity, offering more than double the bandwidth and over 3.6 times the high-bandwidth memory (HBM) capacity. The gains, however, are not without drawbacks. The Huawei system is predicted to be 2.3 times less efficient per floating point operation than Nvidia’s GB200 and has lower power efficiency per unit of memory bandwidth and capacity. Despite the lower performance per watt, Huawei’s system still provides the infrastructure needed to train advanced AI models at scale. Sources indicate that China’s largest chip foundry, SMIC, is producing some of the main components for the 910C using its 7nm N+2 process. Yield levels remain a concern, however, and some of the 910C units reportedly include chips produced by TSMC for Chinese firm Sophgo. Huawei has denied using TSMC-made parts. The US Commerce Department is currently investigating the relationship between TSMC and Sophgo after a Sophgo-designed chip was found in Huawei’s earlier 910B processor. TSMC has maintained that it has not supplied Huawei since 2020 and continues to comply with export regulations. In late 2023, Huawei began distributing early samples of the 910C to selected technology firms and opened its order books. Consulting firm Albright Stonebridge Group suggested the chip is likely to become the go-to choice for Chinese companies building large AI models or deploying inference capacity, given the ongoing export controls on US-made chips. While the Huawei Ascend 910C may not match Nvidia in power efficiency or process technology, it signals a broader trend. Chinese technology firms are developing homegrown alternatives to foreign components, even if it means using less advanced methods to achieve similar outcomes. As global AI demand surges and export restrictions tighten, Huawei’s ability to deliver a scalable AI hardware solution domestically could help shape China’s artificial intelligence future – especially as developers look to secure long-term supply chains and reduce exposure to geopolitical risk. (Photo viaUnsplash) See also:Huawei’s AI hardware breakthrough challenges Nvidia’s dominance Want to learn more about AI and big data from industry leaders? Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Muhammad Zulhusni Journalist April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/huawei-to-begin-mass-shipments-ascend-910c-us-curbs/
The evolution of harmful content detection: Manual moderation to AI,"April 22, 2025","AI News is part of the TechForge Publications series TechForge Artificial Intelligence Vinish Kapoor April 22, 2025 Share this story: Tags: Categories:: The battle to keep online spaces safe and inclusive continues to evolve. As digital platforms multiply and user-generated content expands very quickly, the need for effectiveharmful content detectionbecomes paramount. What once relied solely on the diligence of human moderators has given way to agile, AI-powered tools reshaping how communities and organisations manage toxic behaviours in words and visuals. Early days of content moderation saw human teams tasked with combing through vast amounts of user-submitted materials – flagging hate speech, misinformation, explicit content, and manipulated images. While human insight brought valuable context and empathy, the sheer volume of submissions naturally outstripped what manual oversight could manage. Burnout among moderators also raised serious concerns. The result was delayed interventions, inconsistent judgment, and myriad harmful messages left unchecked. To address scale and consistency, early stages of automated detection software surfaced – chiefly, keyword filters and naïve algorithms. These could scan quickly for certain banned terms or suspicious phrases, offering some respite for moderation teams. However, contextless automation brought new challenges: benign messages were sometimes mistaken for malicious ones due to crude word-matching, and evolving slang frequently bypassed protection. Artificial intelligence changed this field. Using deep learning, machine learning, and neural networks, AI-powered systems now process vast and diverse streams of data with previously impossible nuance. Rather than just flagging keywords, algorithms can detect intent, tone, and emergent abuse patterns. Among the most pressing concerns are harmful or abusive messages on social networks, forums, and chats. Modern solutions, like theAI-powered hate speech detectordeveloped byVinish Kapoor, demonstrate how free, online tools have democratised access to reliable content moderation. The platform allows anyone to analyse a string of text for hate speech, harassment, violence, and other manifestations of online toxicity instantly – without technical know-how, subscriptions, or concern for privacy breaches. Such a detector moves beyond outdated keyword alarms by evaluating semantic meaning and context, so reducing false positives and highlighting sophisticated or coded abusive language drastically. The detection process adapts as internet linguistics evolve. It’s not just text that requires vigilance. Images, widely shared on news feeds and messaging apps, pose unique risks: manipulated visuals often aim to misguide audiences or propagate conflict. AI-creators now offer robust tools forimage anomaly detection. Here, AI algorithms scan for inconsistencies like noise patterns, flawed shadows, distorted perspective, or mismatches between content layers – common signals of editing or manufacture. The offerings stand out not only for accuracy but for sheer accessibility. Their completely free resources, overcome lack of technical requirements, and offer a privacy-centric approach that allows hobbyists, journalists, educators, and analysts to safeguard image integrity with remarkable simplicity. Modern AI solutions introduce vital advantages into the field: The future of digital safety likely hinges on greater collaboration between intelligent automation and skilled human input. As AI models learn from more nuanced examples, their ability to curb emergent forms of harm will expand. Yet human oversight remains essential for sensitive cases demanding empathy, ethics, and social understanding. With open, free solutions widely available and enhanced by privacy-first models, everyone from educators to business owners now possesses the tools to protect digital exchanges at scale – whether safeguarding group chats, user forums, comment threads, or email chains. Harmful content detection has evolved dramatically – from slow, error-prone manual reviews to instantaneous, sophisticated, and privacy-conscious AI. Today’s innovations strike a balance between broad coverage, real-time intervention, and accessibility, reinforcing the idea that safer, more positive digital environments are in everyone’s reach – no matter their technical background or budget. (Image source: Pexels) Vinish Kapoor Vinish Kapoor April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/the-evolution-of-harmful-content-detection-manual-moderation-to-ai/
Google launches A2A as HyperCycle advances AI agent interoperability,"April 22, 2025","AI News is part of the TechForge Publications series TechForge Artificial Intelligence AI News April 22, 2025 Share this story: Tags: Categories:: AI agents handle increasingly complex and recurring tasks, such as planning supply chains and ordering equipment. As organisations deploy more agents developed by different vendors on different frameworks, agents can end up siloed, unable to coordinate or communicate. Lack of interoperability remains a challenge for organisations, with different agents making conflicting recommendations. It’s difficult to create standardised AI workflows, and agent integration require middleware, adding more potential failure points and layers of complexity. Google unveiled its Agent2Agent (A2A) protocol at Cloud Next 2025 in an effort to standardise communication between diverse AI agents. A2A is an open protocol that allows independent AI agents to communicate and cooperate. It complements Anthropic’s Model Context Protocol (MCP), which provides models with context and tools. MCP connects agents to tools and other resources, and A2A connects agents to other agents. Google’s new protocol facilitates collaboration among AI agents on different platforms and vendors, and ensures secure, real-time communication, and task coordination. The two roles in an A2A-enabled system are a client agent and a remote agent. The client initiates a task to achieve a goal or on behalf of a user, It makes requests which the remote agent receives and acts on. Depending on who initiates the communication, an agent can be a client agent in one interaction and a remote agent in another. The protocol defines a standard message format and workflow for the interaction. Tasks are at the heart of A2A, with each task representing a work or conversation unit. The client agent sends the request to the remote agent’s send or task endpoint. The request includes instructions and a unique task ID. The remote agent creates a new task and starts working on it. Google enjoys broad industry support, with contributions frommore than 50 technology partnerslike Intuit, Langchain, MongoDB, Atlassian, Box, Cohere, PayPal, Salesforce, SAP, Workday, ServiceNow, and UKG. Reputable service providers include Capgemini, Cognizant, Accenture, BCG, Deloitte, HCLTech, McKinsey, PwC, TCS, Infosys, KPMG, and Wipro. HyperCycle’s Node Factory framework makes it possible to deploy multiple agents, addressing existing challenges and enabling developers to create reliable, collaborative setups. The decentralised platform is advancing the bold concept of “the internet of AI” and using self-perpetuating nodes and a creative licensing model to enable AI deployments at scale. The framework helps achieve cross-platform interoperability by standardising interactions and supporting agents from different developers so agents can work cohesively, irrespective of origin. The platform’speer-to-peernetwork links agents across an ecosystem, eliminating silos and enabling unified data sharing and coordination across nodes. The self-replicating nodes can scale, reducing infrastructure needs and distributing computational loads. Each Node Factory replicates up to ten times, with the number of nodes in the Factory doubling each time. Users can buy and operate Node Factories at ten different levels. Growth enhances each Factory’s capacity, fulfilling increasing demand for AI services. One node might host a communication-focused agent, while another supports a data analysis agent. Developers can create custom solutions by crafting multi-agent tools from the nodes they’re using, addressing scalability issues and siloed environments. HyperCycle’s Node Factory operates in a network using Toda/IP architecture, which parallels TCP/IP. The network encompasses hundreds of thousands of nodes, letting developers integrate third-party agents. A developer can enhance function by incorporating a third-party analytics agent, sharing intelligence, and promoting collaboration across the network. According to Toufi Saliba, HyperCycle’s CEO, the exciting development from Google around A2A represents a major milestone for his agent cooperation project. The news supports his vision of interoperable, scalable AI agents. In an X post,he saidmany more AI agents will now be able to access the nodes produced by HyperCycle Factories. Nodes can be plugged into any A2A, giving each AI agent in Google Cloud (and its 50+ partners) near-instant access to AWS agents, Microsoft agents, and the entire internet of AI. Saliba’s statement highlights A2A’s potential and its synergy with HyperCycle’s mission. HyperCycle’s Layer 0++ blockchain infrastructure offers security and speed, and complements A2A by providing a decentralised, secure infrastructure for AI agent interactions. Layer 0++ is an innovative blockchain operating on Toda/IP, which divides network packets into smaller pieces and distributes them across nodes. It can also extend the usability of other blockchains by bridging to them, which means HyperCycle can enhance the functionality of Bitcoin, Ethereum, Avalanche, Cosmos, Cardano, Polygon, Algorand, and Polkadot rather than compete with those blockchains. HyperCycle has potential in areas like DeFi, swarm AI, media ratings and rewards, decentralised payments, and computer processing. Swarm AI is a collective intelligence system where individual agents collaborate to solve complicated problems. They can interoperate more often with HyperCycle, leading to lightweight agents carrying out complex internal processes. The HyperCycle platform can improve ratings and rewards in media networks through micro-transactions. The ability to perform high-frequency, high-speed, low-cost, on-chain trading presents innumerable opportunities in DeFi. It can streamline decentralised payments and computer processing by increasing the speed and reducing the cost of blockchain transactions. HyperCycle’s efforts to improve access to information precede Google’s announcement. In January 2025,the platform announcedit had launched a joint initiative with YMCA – an AI app called Hyper-Y that will connect 64 million people in 12,000 YMCA locations across 120 countries, providing staff, members, and volunteers with access to information from the global network. Google hopes its protocol will pave the way for collaboration to solve complex problems and will build the protocol with the community, in the open. A2A was released as open-source with plans to set up contribution pathways. HyperCycle’s innovations aim to enable collaborative problem-solving by connecting AI to a global network of specialised abilities as A2A standardises communication between agents regardless of their vendor or build, so introducing more collaborative multi-agent ecosystems. A2A and Hypercycle bring ease of use, modularity, scalability, and security to AI agent systems. They can unlock a new era of agent interoperability, creating more flexible and powerful agentic systems. (Image source: Unsplash) AI News AI News April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/google-launches-a2a-as-hypercycle-advances-ai-agent-interoperability/
"Red Hat on open, small language models for responsible, practical AI","April 22, 2025","AI News is part of the TechForge Publications series TechForge Artificial Intelligence Joe Green April 22, 2025 Share this story: Tags: Categories:: As geopolitical events shape the world, it’s no surprise that they affect technology too – specifically, in the ways that the current AI market is changing, alongside its accepted methodology, how it’s developed, and the ways it’s put to use in the enterprise. The expectations of results from AI are balanced at present with real-world realities. And there remains a good deal of suspicion about the technology, again in balance with those who are embracing it even in its current nascent stages. The closed-loop nature of the well-known LLMs is being challenged by instances like Llama, DeepSeek, and Baidu’s recently-released Ernie X1. In contrast, open source development provides transparency and the ability to contribute back, which is more in tune with the desire for “responsible AI”: a phrase that encompasses the environmental impact of large models, how AIs are used, what comprises their learningcorpora, and issues around data sovereignty, language, and politics. As the company that’s demonstrated the viability of an economically-sustainable open source development model for its business,Red Hatwants to extend its open, collaborative, and community-driven approach to AI. We spoke recently to Julio Guijarro, the CTO for EMEA at Red Hat, about the organisation’s efforts to unlock the undoubted power of generative AI models in ways that bring value to the enterprise, in a manner that’s responsible, sustainable, and as transparent as possible. Julio underlined how much education is still needed in order for us to more fully understand AI, stating, “Given the significant unknowns about AI’s inner workings, which are rooted in complex science and mathematics, it remains a ‘black box’ for many. This lack of transparency is compounded where it has been developed in largely inaccessible, closed environments.” There are also issues with language (European and Middle-Eastern languages are very much under-served), data sovereignty, and fundamentally, trust. “Data is an organisation’s most valuable asset, and businesses need to make sure they are aware of the risks of exposing sensitive data to public platforms with varying privacy policies.” The Red Hat response Red Hat’s response to global demand for AI has been to pursue what it feels will bring most benefit to end-users, and remove many of the doubts and caveats that are quickly becoming apparent when thede factoAI services are deployed. One answer, Julio said, is small language models, running locally or in hybrid clouds, on non-specialist hardware, and accessing local business information. SLMs are compact, efficient alternatives to LLMs, designed to deliver strong performance for specific tasks while requiring significantly fewer computational resources. There are smaller cloud providers that can be utilised to offload some compute, but the key is having the flexibility and freedom to choose to keep business-critical information in-house, close to the model, if desired. That’s important, because information in an organisation changes rapidly. “One challenge with large language models is they can get obsolete quickly because the data generation is not happening in the big clouds. The data is happening next to you and your business processes,” he said. There’s also the cost. “Your customer service querying an LLM can present a significant hidden cost – before AI, you knew that when you made a data query, it had a limited and predictable scope. Therefore, you could calculate how much that transaction could cost you. In the case of LLMs, they work on an iterative model. So the more you use it, the better its answer can get, and the more you like it, the more questions you may ask. And every interaction is costing you money. So the same query that before was a single transaction can now become a hundred, depending on who and how is using the model. When you are running a model on-premise, you can have greater control, because the scope is limited by the cost of your own infrastructure, not by the cost of each query.” Organisations needn’t brace themselves for a procurement round that involves writing a huge cheque for GPUs, however. Part of Red Hat’s current work is optimising models (in the open, of course) to run on more standard hardware. It’s possible because the specialist models that many businesses will use don’t need the huge, general-purpose datacorpusthat has to be processed at high cost with every query. “A lot of the work that is happening right now is people looking into large models and removing everything that is not needed for a particular use case. If we want to make AI ubiquitous, it has to be through smaller language models. We are also focused on supporting and improving vLLM (the inference engine project) to make sure people can interact with all these models in an efficient and standardised way wherever they want: locally, at the edge or in the cloud,” Julio said. Keeping it small Using and referencing local data pertinent to the user means that the outcomes can be crafted according to need. Julio cited projects in the Arab- and Portuguese-speaking worlds that wouldn’t be viable using the English-centric household name LLMs. There are a couple of other issues, too, that early adopter organisations have found in practical, day-to-day use LLMs. The first is latency – which can be problematic in time-sensitive or customer-facing contexts. Having the focused resources and relevantly-tailored results just a network hop or two away makes sense. Secondly, there is the trust issue: an integral part of responsible AI. Red Hat advocates for open platforms, tools, and models so we can move towards greater transparency, understanding, and the ability for as many people as possible to contribute. “It is going to be critical for everybody,” Julio said. “We are building capabilities to democratise AI, and that’s not only publishing a model, it’s giving users the tools to be able to replicate them, tune them, and serve them.” Red Hat recentlyacquired Neural Magicto help enterprises more easily scale AI, to improve performance of inference, and to provide even greater choice and accessibility of how enterprises build and deploy AI workloads with thevLLMproject for open model serving. Red Hat, together with IBM Research, also releasedInstructLabto open the door to would-be AI builders who aren’t data scientists but who have the right business knowledge. There’s a great deal of speculation around if, or when, the AI bubble might burst, but such conversations tend to gravitate to the economic reality that the big LLM providers will soon have to face. Red Hat believes that AI has a future in a use case-specific and inherently open source form, a technology that will make business sense and that will be available to all. To quote Julio’s boss, Matt Hicks (CEO of Red Hat), “The future of AI is open.” Supporting Assets: Tech Journey: Adopt and scale AI Joe Green Commercial Editor April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/red-hat-on-open-small-language-models-for-responsible-practical-ai/
Meta FAIR advances human-like AI with five major releases,"April 17, 2025","AI News is part of the TechForge Publications series TechForge Artificial Intelligence Ryan Daws April 17, 2025 Share this story: Tags: Categories:: The Fundamental AI Research (FAIR) team at Meta has announced five projects advancing the company’s pursuit of advanced machine intelligence (AMI). The latest releases fromMetafocus heavily on enhancing AI perception – the ability for machines to process and interpret sensory information – alongside advancements in language modelling, robotics, and collaborative AI agents. Meta stated its goal involves creating machines “that are able to acquire, process, and interpret sensory information about the world around us and are able to use this information to make decisions with human-like intelligence and speed.” The five new releases represent diverse but interconnected efforts towards achieving this ambitious goal. Central to the new releases is the Perception Encoder, described as a large-scale vision encoder designed to excel across various image and video tasks. Vision encoders function as the “eyes” for AI systems, allowing them to understand visual data. Meta highlights the increasing challenge of building encoders that meet the demands of advanced AI, requiring capabilities that bridge vision and language, handle both images and videos effectively, and remain robust under challenging conditions, including potential adversarial attacks. The ideal encoder, according to Meta, should recognise a wide array of concepts while distinguishing subtle details—citing examples like spotting “a stingray burrowed under the sea floor, identifying a tiny goldfinch in the background of an image, or catching a scampering agouti on a night vision wildlife camera.” Meta claims the Perception Encoder achieves “exceptional performance on image and video zero-shot classification and retrieval, surpassing all existing open source and proprietary models for such tasks.” Furthermore, its perceptual strengths reportedly translate well to language tasks. When aligned with a large language model (LLM), the encoder is said to outperform other vision encoders in areas like visual question answering (VQA), captioning, document understanding, and grounding (linking text to specific image regions). It also reportedly boosts performance on tasks traditionally difficult for LLMs, such as understanding spatial relationships (e.g., “if one object is behind another”) or camera movement relative to an object. “As Perception Encoder begins to be integrated into new applications, we’re excited to see how its advanced vision capabilities will enable even more capable AI systems,” Meta said. Complementing the encoder is the Perception Language Model (PLM), an open and reproducible vision-language model aimed at complex visual recognition tasks. PLM was trained using large-scale synthetic data combined with open vision-language datasets, explicitly without distilling knowledge from external proprietary models. Recognising gaps in existing video understanding data, the FAIR team collected 2.5 million new, human-labelled samples focused on fine-grained video question answering and spatio-temporal captioning. Meta claims this forms the “largest dataset of its kind to date.” PLM is offered in 1, 3, and 8 billion parameter versions, catering to academic research needs requiring transparency. Alongside the models, Meta is releasing PLM-VideoBench, a new benchmark specifically designed to test capabilities often missed by existing benchmarks, namely “fine-grained activity understanding and spatiotemporally grounded reasoning.” Meta hopes the combination of open models, the large dataset, and the challenging benchmark will empower the open-source community. Bridging the gap between language commands and physical action is Meta Locate 3D. This end-to-end model aims to allow robots to accurately localise objects in a 3D environment based on open-vocabulary natural language queries. Meta Locate 3D processes 3D point clouds directly from RGB-D sensors (like those found on some robots or depth-sensing cameras). Given a textual prompt, such as “flower vase near TV console,” the system considers spatial relationships and context to pinpoint the correct object instance, distinguishing it from, say, a “vase on the table.” The system comprises three main parts: a preprocessing step converting 2D features to 3D featurised point clouds; the 3D-JEPA encoder (a pretrained model creating a contextualised 3D world representation); and the Locate 3D decoder, which takes the 3D representation and the language query to output bounding boxes and masks for the specified objects. Alongside the model, Meta is releasing a substantial new dataset for object localisation based on referring expressions. It includes 130,000 language annotations across 1,346 scenes from the ARKitScenes, ScanNet, and ScanNet++ datasets, effectively doubling existing annotated data in this area. Meta sees this technology as crucial for developing more capable robotic systems, including its own PARTNR robot project, enabling more natural human-robot interaction and collaboration. Following research published in late 2024, Meta is now releasing the model weights for its 8-billion parameter Dynamic Byte Latent Transformer. This architecture represents a shift away from traditional tokenisation-based language models, operating instead at the byte level. Meta claims this approach achieves comparable performance at scale while offering significant improvements in inference efficiency and robustness. Traditional LLMs break text into ‘tokens’, which can struggle with misspellings, novel words, or adversarial inputs. Byte-level models process raw bytes, potentially offering greater resilience. Meta reports that the Dynamic Byte Latent Transformer “outperforms tokeniser-based models across various tasks, with an average robustness advantage of +7 points (on perturbed HellaSwag), and reaching as high as +55 points on tasks from the CUTE token-understanding benchmark.” By releasing the weights alongside the previously shared codebase, Meta encourages the research community to explore this alternative approach to language modelling. The final release, Collaborative Reasoner, tackles the complex challenge of creating AI agents that can effectively collaborate with humans or other AIs. Meta notes that human collaboration often yields superior results, and aims to imbue AI with similar capabilities for tasks like helping with homework or job interview preparation. Such collaboration requires not just problem-solving but also social skills like communication, empathy, providing feedback, and understanding others’ mental states (theory-of-mind), often unfolding over multiple conversational turns. Current LLM training and evaluation methods often neglect these social and collaborative aspects. Furthermore, collecting relevant conversational data is expensive and difficult. Collaborative Reasoner provides a framework to evaluate and enhance these skills. It includes goal-oriented tasks requiring multi-step reasoning achieved through conversation between two agents. The framework tests abilities like disagreeing constructively, persuading a partner, and reaching a shared best solution. Meta’s evaluations revealed that current models struggle to consistently leverage collaboration for better outcomes. To address this, they propose a self-improvement technique using synthetic interaction data where an LLM agent collaborates with itself. Generating this data at scale is enabled by a new high-performance model serving engine called Matrix. Using this approach on maths, scientific, and social reasoning tasks reportedly yielded improvements of up to 29.4% compared to the standard ‘chain-of-thought’ performance of a single LLM. By open-sourcing the data generation and modelling pipeline, Meta aims to foster further research into creating truly “social agents that can partner with humans and other agents.” These five releases collectively underscore Meta’s continued heavy investment in fundamental AI research, particularly focusing on building blocks for machines that can perceive, understand, and interact with the world in more human-like ways. See also:Meta will train AI models using EU user data Want to learn more about AI and big data from industry leaders?Check outAI & Big Data Expotaking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events includingIntelligent Automation Conference,BlockX,Digital Transformation Week, andCyber Security & Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForgehere. Ryan Daws Senior Editor April 25, 2025 April 25, 2025 April 24, 2025 April 24, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence Military Applications Applications Applications April 24, 2025 AGI April 24, 2025 Quantum Computing April 24, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part ofTechForge Notifications",https://www.artificialintelligence-news.com/news/meta-fair-advances-human-like-ai-five-major-releases/
